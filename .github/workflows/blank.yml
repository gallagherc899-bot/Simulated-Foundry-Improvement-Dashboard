# streamlit_app.py
# Run with: streamlit run streamlit_app.py

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import os
import numpy as np
import pandas as pd
import streamlit as st

from dateutil.relativedelta import relativedelta
from scipy.stats import wilcoxon

from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import brier_score_loss, accuracy_score

# -----------------------------
# Page / constants
# -----------------------------
st.set_page_config(
Â  Â  page_title="Simulated Foundry Scrap Risk Dashboard â€” Actionable Insights",
Â  Â  layout="wide"
)

RANDOM_STATE = 42
DEFAULT_ESTIMATORS = 180
MIN_SAMPLES_LEAF = 2

S_GRID = np.linspace(0.6, 1.2, 13)
GAMMA_GRID = np.linspace(0.5, 1.2, 15)
# TOP_K_PARETO is no longer used for dynamic 80% cutoff, but kept as a fallback.
TOP_K_PARETO = 8Â 
# Set rate cols to always be true for Pareto output (manager requirement)
USE_RATE_COLS_PERMANENT = TrueÂ 


# -----------------------------
# Helpers - MODIFIED for 80% Pareto Rule
# -----------------------------
@st.cache_data(show_spinner=False)
def load_and_clean(csv_path: str) -> pd.DataFrame:
Â  Â  df = pd.read_csv(csv_path)
Â  Â  df.columns = (
Â  Â  Â  Â  df.columns.str.strip()
Â  Â  Â  Â  .str.lower()
Â  Â  Â  Â  .str.replace(" ", "_")
Â  Â  Â  Â  .str.replace("(", "", regex=False)
Â  Â  Â  Â  .str.replace(")", "", regex=False)
Â  Â  Â  Â  .str.replace("#", "num", regex=False)
Â  Â  )
Â  Â  needed = ["part_id", "week_ending", "scrap%", "order_quantity", "piece_weight_lbs"]
Â  Â  missing = [c for c in needed if c not in df.columns]
Â  Â  if missing:
Â  Â  Â  Â  raise ValueError(f"Missing column(s): {missing}")

Â  Â  df["week_ending"] = pd.to_datetime(df["week_ending"], errors="coerce")
Â  Â  df = df.dropna(subset=needed).copy()

Â  Â  for c in ["scrap%", "order_quantity", "piece_weight_lbs"]:
Â  Â  Â  Â  df[c] = pd.to_numeric(df[c], errors="coerce")
Â  Â  df = df.dropna(subset=["scrap%", "order_quantity", "piece_weight_lbs"]).copy()

Â  Â  if "pieces_scrapped" not in df.columns:
Â  Â  Â  Â  df["pieces_scrapped"] = np.round((df["scrap%"].clip(lower=0) / 100.0) * df["order_quantity"]).astype(int)

Â  Â  df = df.sort_values("week_ending").reset_index(drop=True)
Â  Â  return df

def time_split(df: pd.DataFrame, train_frac=0.60, calib_frac=0.20):
Â  Â  n = len(df)
Â  Â  t_end = int(train_frac * n)
Â  Â  c_end = int((train_frac + calib_frac) * n)

Â  Â  df_train = df.iloc[:t_end].copy()
Â  Â  df_calib = df.iloc[t_end:c_end].copy()
Â  Â  df_testÂ  = df.iloc[c_end:].copy()

Â  Â  # prevent part leakage
Â  Â  train_parts = set(df_train.part_id.unique())
Â  Â  # Exclude parts seen in train from calib/test to ensure proper backtesting validation
Â  Â  df_calib = df_calib[~df_calib.part_id.isin(train_parts)].copy()
Â  Â  calib_parts = set(df_calib.part_id.unique())
Â  Â  df_testÂ  = df_test[~df_test.part_id.isin(train_parts.union(calib_parts))].copy()
Â  Â  return df_train, df_calib, df_test

def compute_mtbf_on_train(df_train: pd.DataFrame, thr_label: float) -> pd.DataFrame:
Â  Â  t = df_train.copy()
Â  Â  t["scrap_flag"] = (t["scrap%"] > thr_label).astype(int)
Â  Â  mtbf = t.groupby("part_id").agg(
Â  Â  Â  Â  total_runs=("scrap%", "count"),
Â  Â  Â  Â  failures=("scrap_flag", "sum")
Â  Â  )
Â  Â  mtbf["mttf_scrap"] = mtbf["total_runs"] / mtbf["failures"].replace(0, np.nan)
Â  Â  mtbf["mttf_scrap"] = mtbf["mttf_scrap"].fillna(mtbf["total_runs"])
Â  Â  return mtbf[["mttf_scrap"]]

def attach_train_features(df_sub, mtbf_train, part_freq_train, default_mtbf, default_freq):
Â  Â  s = df_sub.merge(mtbf_train, on="part_id", how="left")
Â  Â  s["mttf_scrap"] = s["mttf_scrap"].fillna(default_mtbf)
Â  Â  s = s.merge(part_freq_train.rename("part_freq"), left_on="part_id", right_index=True, how="left")
Â  Â  s["part_freq"] = s["part_freq"].fillna(default_freq)
Â  Â  return s

def make_xy(df, thr_label: float, use_rate_cols: bool):
Â  Â  feats = ["order_quantity", "piece_weight_lbs", "mttf_scrap", "part_freq"]
Â  Â  if use_rate_cols:
Â  Â  Â  Â  feats += [c for c in df.columns if c.endswith("_rate")]
Â  Â  X = df[feats].copy()
Â  Â  y = (df["scrap%"] > thr_label).astype(int)
Â  Â  return X, y, feats

@st.cache_resource(show_spinner=True)
def train_and_calibrate(X_train, y_train, X_calib, y_calib, n_estimators: int):
Â  Â  rf = RandomForestClassifier(
Â  Â  Â  Â  n_estimators=n_estimators,
Â  Â  Â  Â  min_samples_leaf=MIN_SAMPLES_LEAF,
Â  Â  Â  Â  class_weight="balanced",
Â  Â  Â  Â  random_state=RANDOM_STATE,
Â  Â  Â  Â  n_jobs=-1
Â  Â  ).fit(X_train, y_train)

Â  Â  has_both = (y_calib.sum() > 0) and (y_calib.sum() < len(y_calib))
Â  Â  method = "isotonic" if has_both and len(y_calib) > 500 else "sigmoid"
Â  Â  try:
Â  Â  Â  Â  cal = CalibratedClassifierCV(estimator=rf, method=method, cv="prefit").fit(X_calib, y_calib)
Â  Â  except Exception:
Â  Â  Â  Â  cal = CalibratedClassifierCV(estimator=rf, method="sigmoid", cv="prefit").fit(X_calib, y_calib)
Â  Â  Â  Â  method = "sigmoid"
Â  Â  return rf, cal, method

def tune_s_gamma_on_validation(p_val_raw, y_val, part_ids_val, part_scale,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â s_grid=S_GRID, gamma_grid=GAMMA_GRID):
Â  Â  """Minimize Brier on validation for p_adj = p_raw * s * (part_scale^gamma)."""
Â  Â  if not len(p_val_raw):
Â  Â  Â  Â  Â  return {"brier_val": np.nan, "s": 1.0, "gamma": 1.0}
Â  Â Â 
Â  Â  ps = part_scale.reindex(part_ids_val).fillna(1.0).to_numpy(dtype=float)
Â  Â  best = (np.inf, 1.0, 1.0)
Â  Â  for s in s_grid:
Â  Â  Â  Â  for g in gamma_grid:
Â  Â  Â  Â  Â  Â  p_adj = np.clip(p_val_raw * (s * (ps ** g)), 0, 1)
Â  Â  Â  Â  Â  Â  score = brier_score_loss(y_val, p_adj)
Â  Â  Â  Â  Â  Â  if score < best[0]:
Â  Â  Â  Â  Â  Â  Â  Â  best = (score, s, g)
Â  Â  return {"brier_val": best[0], "s": best[1], "gamma": best[2]}

def prior_shift_logit(p_raw, src_prev, tgt_prev):
Â  Â  """Saerensâ€“Latinne prior correction on logits."""
Â  Â  p = np.clip(p_raw, 1e-6, 1-1e-6)
Â  Â  logit = np.log(p/(1-p))
Â  Â  delta = np.log(np.clip(tgt_prev,1e-6,1-1e-6)) - np.log(np.clip(1-tgt_prev,1e-6,1))
Â  Â  delta -= np.log(np.clip(src_prev,1e-6,1-1e-6)) - np.log(np.clip(1-src_prev,1e-6,1))
Â  Â  p_adj = 1/(1 + np.exp(-(logit + delta)))
Â  Â  return np.clip(p_adj, 1e-6, 1-1e-6)

# Exceedance baselines at current threshold
def compute_part_exceedance_baselines(df_train: pd.DataFrame, thr_label: float):
Â  Â  """Per-part prevalence of exceeding the threshold, and a scale vs global prevalence."""
Â  Â  part_prev = (
Â  Â  Â  Â  df_train.assign(exceed=(df_train["scrap%"] > thr_label).astype(int))
Â  Â  Â  Â  Â  Â  Â  Â  .groupby("part_id")["exceed"].mean()
Â  Â  Â  Â  Â  Â  Â  Â  .clip(lower=1e-6, upper=0.999)
Â  Â  )
Â  Â  global_prev = float(part_prev.mean()) if len(part_prev) else 0.5
Â  Â  part_scale = (part_prev / max(global_prev, 1e-6)).fillna(1.0).clip(lower=0.25, upper=4.0)
Â  Â  return part_prev, part_scale, global_prev

def build_input_row_for_part(selected_part: int,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â quantity: float,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â weight: float,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â mttf_value: float,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â part_freq_value: float,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â FEATURES: list,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â df_train: pd.DataFrame) -> pd.DataFrame:
Â  Â  """
Â  Â  Build a one-row dataframe aligned to FEATURES.
Â  Â  For *_rate columns, fill with the part's historical mean (fallback = global mean).
Â  Â  """
Â  Â  base = {"order_quantity": quantity,
Â  Â  Â  Â  Â  Â  "piece_weight_lbs": weight,
Â  Â  Â  Â  Â  Â  "mttf_scrap": mttf_value,
Â  Â  Â  Â  Â  Â  "part_freq": part_freq_value}

Â  Â  row = pd.DataFrame([base], columns=["order_quantity", "piece_weight_lbs", "mttf_scrap", "part_freq"])

Â  Â  rate_cols = [c for c in FEATURES if c.endswith("_rate")]
Â  Â  if rate_cols:
Â  Â  Â  Â  part_hist = df_train[df_train["part_id"] == selected_part][rate_cols].mean()
Â  Â  Â  Â  global_hist = df_train[rate_cols].mean()
Â  Â  Â  Â  filled = part_hist.fillna(global_hist).fillna(0.0)
Â  Â  Â  Â  for c in rate_cols:
Â  Â  Â  Â  Â  Â  row[c] = float(filled.get(c, float(global_hist.get(c, 0.0))))
Â  Â  # Reorder to FEATURES exactly
Â  Â  row = row.reindex(columns=FEATURES, fill_value=0.0)
Â  Â  return row

def local_defect_drivers(calibrated_model,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â input_row: pd.DataFrame,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â FEATURES: list,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â df_train: pd.DataFrame,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â strategy: str = "p75",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â k: int = 100) -> pd.DataFrame: # Increased k for intermediate calculation
Â  Â  """
Â  Â  Compute a local 'predicted Pareto' by measuring the positive delta in raw calibrated probability.
Â  Â  Now implements a dynamic stop at the first defect where cumulative_% meets or exceeds 80%.
Â  Â  """
Â  Â  rate_cols = [c for c in FEATURES if c.endswith("_rate")]
Â  Â  if not rate_cols:
Â  Â  Â  Â  return pd.DataFrame(columns=["defect", "delta_prob", "share_%", "cumulative_%"])

Â  Â  base_p = float(calibrated_model.predict_proba(input_row)[0, 1])

Â  Â  deltas = []
Â  Â  for col in rate_cols:
Â  Â  Â  Â  tmp = input_row.copy()
Â  Â  Â  Â  if strategy == "p75":
Â  Â  Â  Â  Â  Â  hi = float(np.nanpercentile(df_train[col].values, 75))
Â  Â  Â  Â  Â  Â  tmp[col] = hi
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  tmp[col] = float(input_row[col].iloc[0]) + 0.02

Â  Â  Â  Â  new_p = float(calibrated_model.predict_proba(tmp)[0, 1])
Â  Â  Â  Â  delta = max(0.0, new_p - base_p)Â  # only positive risk drivers for Pareto
Â  Â  Â  Â  deltas.append((col, delta))

Â  Â  dd = pd.DataFrame(deltas, columns=["defect", "delta_prob"])
Â  Â  dd = dd[dd["delta_prob"] > 1e-6] # Filter out defects with negligible impact
Â  Â  dd = dd.sort_values("delta_prob", ascending=False).head(k)
Â  Â  total = float(dd["delta_prob"].sum())
Â  Â Â 
Â  Â  if total > 0:
Â  Â  Â  Â  dd["share_%"] = dd["delta_prob"] / total * 100.0
Â  Â  Â  Â  dd["cumulative_%"] = dd["share_%"].cumsum()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- 80% Pareto Rule Implementation ---
Â  Â  Â  Â  stop_index_loc = dd[dd["cumulative_%"] >= 80.0].index.min()
Â  Â  Â  Â  if pd.isna(stop_index_loc):
Â  Â  Â  Â  Â  Â  Â  return dd
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Get the index position of the stop_index_loc, and slice up to that position (inclusive)
Â  Â  Â  Â  stop_pos = dd.index.get_loc(stop_index_loc)
Â  Â  Â  Â  return dd.iloc[:stop_pos + 1]
Â  Â  Â  Â Â 
Â  Â  else:
Â  Â  Â  Â  dd["share_%"] = 0.0
Â  Â  Â  Â  dd["cumulative_%"] = 0.0
Â  Â  Â  Â  return dd.head(0)


def historical_defect_pareto_for_part(selected_part: int,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_train: pd.DataFrame,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  k: int = 100) -> pd.DataFrame: # Increased k for intermediate calculation
Â  Â  """
Â  Â  Top historical defect rates (means) for the part, dynamically stopping at 80% cumulative share.
Â  Â  """
Â  Â  rate_cols = [c for c in df_train.columns if c.endswith("_rate")]
Â  Â  if not rate_cols:
Â  Â  Â  Â  return pd.DataFrame(columns=["defect", "mean_rate", "share_%", "cumulative_%"])

Â  Â  part_hist = df_train[df_train["part_id"] == selected_part]
Â  Â  if part_hist.empty:
Â  Â  Â  Â  return pd.DataFrame(columns=["defect", "mean_rate", "share_%", "cumulative_%"])

Â  Â  means = part_hist[rate_cols].mean().fillna(0.0)
Â  Â  means = means[means > 1e-6] # Filter out negligible mean rates
Â  Â  means = means.sort_values(ascending=False).head(k)
Â  Â  total = float(means.sum())
Â  Â Â 
Â  Â  out = pd.DataFrame({"defect": means.index,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "mean_rate": means.values})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  if total > 0:
Â  Â  Â  Â  out["share_%"] = out["mean_rate"] / total * 100.0
Â  Â  Â  Â  out["cumulative_%"] = out["share_%"].cumsum()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- 80% Pareto Rule Implementation ---
Â  Â  Â  Â  stop_index_loc = out[out["cumulative_%"] >= 80.0].index.min()
Â  Â  Â  Â  if pd.isna(stop_index_loc):
Â  Â  Â  Â  Â  Â  Â  return out
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Get the index position of the stop_index_loc, and slice up to that position (inclusive)
Â  Â  Â  Â  stop_pos = out.index.get_loc(stop_index_loc)
Â  Â  Â  Â  return out.iloc[:stop_pos + 1]
Â  Â  Â  Â Â 
Â  Â  else:
Â  Â  Â  Â  out["share_%"] = 0.0
Â  Â  Â  Â  out["cumulative_%"] = 0.0
Â  Â  Â  Â  return out.head(0)

# Add risk alert summary box based on corrected_p
def get_alert_summary(prob):
Â  Â  # This function remains unchanged as the risk tiers are based on best practices.
Â  Â  if prob >= 0.90:
Â  Â  Â  Â  color = "red"
Â  Â  Â  Â  status = "ğŸ”´ High Alert for Scrap"
Â  Â  Â  Â  message = (
Â  Â  Â  Â  Â  Â  "Strong likelihood of scrap. Monitor **tear_up_rate** and **runout_rate**. "
Â  Â  Â  Â  Â  Â  "Review recent patterns and defects in historical vs. predicted Pareto."
Â  Â  Â  Â  )
Â  Â  elif prob >= 0.80:
Â  Â  Â  Â  color = "orange"
Â  Â  Â  Â  status = "ğŸŸ  Serious Concern for Scrap"
Â  Â  Â  Â  message = (
Â  Â  Â  Â  Â  Â  "Elevated risk of scrap. Investigate predicted defects and ensure quality controls are reinforced."
Â  Â  Â  Â  )
Â  Â  elif prob >= 0.60:
Â  Â  Â  Â  color = "gold"
Â  Â  Â  Â  status = "ğŸŸ¡ Elevated Risk"
Â  Â  Â  Â  message = (
Â  Â  Â  Â  Â  Â  "Moderate chance of scrap. Maintain quality vigilance and inspect critical defect trends."
Â  Â  Â  Â  )
Â  Â  elif prob >= 0.40:
Â  Â  Â  Â  color = "lightgray"
Â  Â  Â  Â  status = "âšª Moderate Risk"
Â  Â  Â  Â  message = (
Â  Â  Â  Â  Â  Â  "Some uncertainty. Proceed with normal inspection and monitor any abnormal trends."
Â  Â  Â  Â  )
Â  Â  elif prob >= 0.20:
Â  Â  Â  Â  color = "lightgreen"
Â  Â  Â  Â  status = "ğŸŸ¢ Low Risk"
Â  Â  Â  Â  message = (
Â  Â  Â  Â  Â  Â  "Low likelihood of scrap. Scrap probability is well within historical norms."
Â  Â  Â  Â  )
Â  Â  else:
Â  Â  Â  Â  color = "green"
Â  Â  Â  Â  status = "ğŸŸ¢ Very Low Risk"
Â  Â  Â  Â  message = (
Â  Â  Â  Â  Â  Â  "Minimal expected scrap. System and process conditions appear highly favorable."
Â  Â  Â  Â  )
Â  Â  return color, status, message


# -----------------------------
# Sidebar - SIMPLIFIED for Foundry Manager
# -----------------------------
st.sidebar.header("Data Source")
# ğŸ¯ TARGETED CHANGE: Update the default value here
csv_path = st.sidebar.text_input("Path to CSV", value="anonymized_parts_simulated_success.csv")

st.sidebar.header("Risk Definition")
# Scrap Threshold is the only adjuster left for the manager (as requested)
thr_label = st.sidebar.slider("Scrap % Threshold (label & MTTFscrap)", 1.0, 15.0, 6.50, 0.5)

# Validation Controls are now exposed in the main sidebar to easily trigger the backtest
st.sidebar.header("Model Validation")
run_validation = st.sidebar.checkbox("Run 6â€“2â€“1 rolling validation (slower)", value=True)


if not os.path.exists(csv_path):
Â  Â  st.error("CSV not found.")
Â  Â  st.stop()

# -----------------------------
# Load and Model Prep (Runs once at app startup)
# -----------------------------
df = load_and_clean(csv_path)

st.title("ğŸ§ª Foundry Scrap Risk Dashboard â€” Actionable Insights")
st.caption("RF + calibrated probs â€¢ **Validation-tuned (s, Î³)** quick-hook â€¢ per-part **exceedance** scaling â€¢ MTTFscrap & reliability â€¢ Historical & Predicted 80% Pareto")

# Global state to hold validation results (for automatic use in the Predict tab)
if "validation_results" not in st.session_state:
Â  Â  st.session_state.validation_results = {}

tabs = st.tabs(["ğŸ”® Predict", "ğŸ“ Validation (6â€“2â€“1)"])

# -----------------------------
# TAB 2: Validation (6â€“2â€“1) - TUNING CONTROLS MOVED HERE
# -----------------------------
with tabs[1]:
Â  Â  st.subheader("Model Hyperparameters & Tuning Controls (Engineer View)")
Â  Â Â 
Â  Â  # Tuning controls MOVED from the old sidebar
Â  Â  c1, c2, c3 = st.columns(3)
Â  Â  with c1:
Â  Â  Â  Â  n_estimators = st.number_input("RandomForest Trees", 80, 600, DEFAULT_ESTIMATORS, 20)
Â  Â  Â  Â  enable_prior_shift = st.checkbox("Enable prior shift (validation âœ test)", value=True)
Â  Â  with c2:
Â  Â  Â  Â  prior_shift_guard = st.slider("Prior-shift guard (max Î” prevalence, pp)", 5, 50, 20, step=5)
Â  Â  with c3:
Â  Â  Â  Â  # Default to NOT use manual hook, but allow engineer to override for testing
Â  Â  Â  Â  use_manual_hook = st.checkbox("Override Quick-Hook (s, Î³)", value=False)
Â  Â  Â  Â  s_manual = st.slider("Manual s", 0.60, 1.20, 1.00, 0.01)
Â  Â  Â  Â  gamma_manual = st.slider("Manual Î³", 0.50, 1.20, 0.50, 0.01)

Â  Â  st.markdown("---")
Â  Â  st.subheader("Rolling 6â€“2â€“1 Backtest with Wilcoxon Significance")
Â  Â Â 
Â  Â  # Store validation results automatically
Â  Â  if run_validation and not st.session_state.validation_results.get("is_complete", False):
Â  Â  Â  Â  with st.spinner("Running rolling evaluationâ€¦"):
Â  Â  Â  Â  Â  Â  rows = []
Â  Â  Â  Â  Â  Â  start_date, end_date = df["week_ending"].min(), df["week_ending"].max()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Simplified for clarity; same logic as original code
Â  Â  Â  Â  Â  Â  s_tuned_list, g_tuned_list = [], []Â 

Â  Â  Â  Â  Â  Â  while start_date + relativedelta(months=(6+2+1)) <= end_date:
Â  Â  Â  Â  Â  Â  Â  Â  # ... (rolling window definitions remain the same) ...
Â  Â  Â  Â  Â  Â  Â  Â  train_end = start_date + relativedelta(months=6)
Â  Â  Â  Â  Â  Â  Â  Â  val_end = train_end + relativedelta(months=2)
Â  Â  Â  Â  Â  Â  Â  Â  test_end = val_end + relativedelta(months=1)

Â  Â  Â  Â  Â  Â  Â  Â  train = df[(df.week_ending >= start_date) & (df.week_ending < train_end)].copy()
Â  Â  Â  Â  Â  Â  Â  Â  val = df[(df.week_ending >= train_end) & (df.week_ending < val_end)].copy()
Â  Â  Â  Â  Â  Â  Â  Â  test = df[(df.week_ending >= val_end) & (df.week_ending < test_end)].copy()

Â  Â  Â  Â  Â  Â  Â  Â  if len(train) < 50 or len(test) < 10:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  start_date += relativedelta(months=1); continue

Â  Â  Â  Â  Â  Â  Â  Â  mtbf_tr = compute_mtbf_on_train(train, thr_label)
Â  Â  Â  Â  Â  Â  Â  Â  default_mtbf = float(mtbf_tr["mttf_scrap"].median()) if len(mtbf_tr) else 1.0
Â  Â  Â  Â  Â  Â  Â  Â  part_freq_tr = train["part_id"].value_counts(normalize=True)
Â  Â  Â  Â  Â  Â  Â  Â  default_freq = float(part_freq_tr.median()) if len(part_freq_tr) else 0.0

Â  Â  Â  Â  Â  Â  Â  Â  train_f = attach_train_features(train, mtbf_tr, part_freq_tr, default_mtbf, default_freq)
Â  Â  Â  Â  Â  Â  Â  Â  val_f = attach_train_features(val, mtbf_tr, part_freq_tr, default_mtbf, default_freq)
Â  Â  Â  Â  Â  Â  Â  Â  test_f = attach_train_features(test, mtbf_tr, part_freq_tr, default_mtbf, default_freq)

Â  Â  Â  Â  Â  Â  Â  Â  X_tr, y_tr, _ = make_xy(train_f, thr_label, USE_RATE_COLS_PERMANENT)
Â  Â  Â  Â  Â  Â  Â  Â  X_va, y_va, _ = make_xy(val_f, thr_label, USE_RATE_COLS_PERMANENT)
Â  Â  Â  Â  Â  Â  Â  Â  X_te, y_te, _ = make_xy(test_f, thr_label, USE_RATE_COLS_PERMANENT)

Â  Â  Â  Â  Â  Â  Â  Â  base = RandomForestClassifier(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_estimators=n_estimators,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_samples_leaf=MIN_SAMPLES_LEAF,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  class_weight="balanced",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  random_state=RANDOM_STATE,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_jobs=-1
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  X_calibfit = pd.concat([X_tr, X_va], axis=0)
Â  Â  Â  Â  Â  Â  Â  Â  y_calibfit = pd.concat([y_tr, y_va], axis=0)
Â  Â  Â  Â  Â  Â  Â  Â  cal = CalibratedClassifierCV(estimator=base, method="sigmoid", cv=3).fit(X_calibfit, y_calibfit)

Â  Â  Â  Â  Â  Â  Â  Â  p_val_rawÂ  = cal.predict_proba(X_va)[:, 1]
Â  Â  Â  Â  Â  Â  Â  Â  p_test_raw = cal.predict_proba(X_te)[:, 1]

Â  Â  Â  Â  Â  Â  Â  Â  if enable_prior_shift and len(p_val_raw) and len(p_test_raw):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prev_src = float(np.clip(p_val_raw.mean(), 1e-6, 1-1e-6))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prev_tgt = float(np.clip((test_f["scrap%"] > thr_label).mean(), 1e-6, 1-1e-6))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  gap_pp = abs(prev_tgt - prev_src) * 100
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if gap_pp <= prior_shift_guard:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_test_raw = prior_shift_logit(p_test_raw, prev_src, prev_tgt)

Â  Â  Â  Â  Â  Â  Â  Â  part_prev_win, part_scale_win, _ = compute_part_exceedance_baselines(train, thr_label)
Â  Â  Â  Â  Â  Â  Â  Â  tune = tune_s_gamma_on_validation(p_val_raw, y_va, val_f["part_id"], part_scale_win, S_GRID, GAMMA_GRID)
Â  Â  Â  Â  Â  Â  Â  Â  s_star, gamma_star = tune["s"], tune["gamma"]
Â  Â  Â  Â  Â  Â  Â  Â  s_tuned_list.append(s_star)
Â  Â  Â  Â  Â  Â  Â  Â  g_tuned_list.append(gamma_star)

Â  Â  Â  Â  Â  Â  Â  Â  pid_test = test_f["part_id"].to_numpy()
Â  Â  Â  Â  Â  Â  Â  Â  ps_testÂ  = part_scale_win.reindex(pid_test).fillna(1.0).to_numpy(dtype=float)
Â  Â  Â  Â  Â  Â  Â  Â  p_test_adj = np.clip(p_test_raw * (s_star * (ps_test ** gamma_star)), 0, 1)

Â  Â  Â  Â  Â  Â  Â  Â  actual_prev = float((test_f["scrap%"] > thr_label).mean())

Â  Â  Â  Â  Â  Â  Â  Â  rows.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "window_start": start_date.date(),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "train_rows": len(train), "test_rows": len(test),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "s_tuned": round(float(s_star),2), "gamma_tuned": round(float(gamma_star),2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "actual_mean": round(actual_prev*100,2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "pred_mean_raw": round(float(np.mean(p_test_raw))*100,2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "pred_mean_adj": round(float(np.mean(p_test_adj))*100,2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "brier_raw": round(brier_score_loss(y_te, p_test_raw),4),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "accuracy_raw": round(accuracy_score(y_te, p_test_raw>0.5),3)
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  start_date += relativedelta(months=1)

Â  Â  Â  Â  Â  Â  results_df = pd.DataFrame(rows)
Â  Â  Â  Â  Â  Â  st.session_state.validation_results["is_complete"] = True
Â  Â  Â  Â  Â  Â  st.session_state.validation_results["results_df"] = results_df
Â  Â  Â  Â  Â  Â  st.session_state.validation_results["s_median"] = np.median(s_tuned_list) if s_tuned_list else 1.0
Â  Â  Â  Â  Â  Â  st.session_state.validation_results["gamma_median"] = np.median(g_tuned_list) if g_tuned_list else 0.5
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # Display validation results
Â  Â  Â  Â  if results_df.empty:
Â  Â  Â  Â  Â  Â  st.warning("No valid rolling windows found.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.dataframe(results_df, use_container_width=True)

Â  Â  Â  Â  Â  Â  def wilcoxon_summary(df, col):
Â  Â  Â  Â  Â  Â  Â  Â  # ... (wilcoxon_summary function remains the same) ...
Â  Â  Â  Â  Â  Â  Â  Â  actual = df["actual_mean"].to_numpy(float)
Â  Â  Â  Â  Â  Â  Â  Â  pred = df[col].to_numpy(float)
Â  Â  Â  Â  Â  Â  Â  Â  rel_err = np.where(actual>0, np.abs(pred-actual)/actual,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â np.where(pred==0, 0.0, 1.0))
Â  Â  Â  Â  Â  Â  Â  Â  gain = np.clip(1.0-rel_err, 0.0, 1.0)
Â  Â  Â  Â  Â  Â  Â  Â  rows=[]
Â  Â  Â  Â  Â  Â  Â  Â  if len(gain)>=10:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for th in [0.50, 0.80, 0.90]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stat, p = wilcoxon(gain-th, alternative="greater")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rows.append([th, gain.mean(), np.median(gain), (gain>=th).mean()*100, stat, p, "âœ…" if p<0.05 else "âŒ"])
Â  Â  Â  Â  Â  Â  Â  Â  return pd.DataFrame(rows, columns=["Threshold","Mean Gain","Median Gain","% Windows â‰¥Threshold","Statistic","p-value","Significant?"])

Â  Â  Â  Â  Â  Â  c1, c2 = st.columns(2)
Â  Â  Â  Â  Â  Â  with c1:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Wilcoxon â€” Adjusted (s, Î³)**")
Â  Â  Â  Â  Â  Â  Â  Â  summ_adj = wilcoxon_summary(results_df, "pred_mean_adj")
Â  Â  Â  Â  Â  Â  Â  Â  if not summ_adj.empty: st.dataframe(summ_adj, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  else: st.info("Need â‰¥10 windows for Wilcoxon.")

Â  Â  Â  Â  Â  Â  with c2:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Wilcoxon â€” Raw (calibrated)**")
Â  Â  Â  Â  Â  Â  Â  Â  summ_raw = wilcoxon_summary(results_df, "pred_mean_raw")
Â  Â  Â  Â  Â  Â  Â  Â  if not summ_raw.empty: st.dataframe(summ_raw, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  else: st.info("Need â‰¥10 windows for Wilcoxon.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown(f"***Median Tuned Quick-Hook: s = {st.session_state.validation_results['s_median']:.2f}, Î³ = {st.session_state.validation_results['gamma_median']:.2f}***")

Â  Â  elif not run_validation:
Â  Â  Â  Â  st.info("Tick **Run 6â€“2â€“1 rolling validation** in the sidebar to compute windows and Wilcoxon tests.")
Â  Â  else:
Â  Â  Â  Â  st.info("Validation results loaded. Un-tick and re-tick the validation box in the sidebar to re-run.")


# -----------------------------
# TAB 1: Predict (Now uses stored validation results)
# -----------------------------
with tabs[0]:
Â  Â  st.subheader("Actionable Scrap Risk Prediction")

Â  Â  df_train, df_calib, df_test = time_split(df)
Â  Â Â 
Â  Â  # --- GET TUNING PARAMS ---
Â  Â  s_star = st.session_state.validation_results.get("s_median", 1.0)
Â  Â  gamma_star = st.session_state.validation_results.get("gamma_median", 0.5)
Â  Â Â 
Â  Â  # Train-only features at current threshold
Â  Â  mtbf_train = compute_mtbf_on_train(df_train, thr_label)
Â  Â  default_mtbf = float(mtbf_train["mttf_scrap"].median()) if len(mtbf_train) else 1.0
Â  Â  part_freq_train = df_train["part_id"].value_counts(normalize=True)
Â  Â  default_freq = float(part_freq_train.median()) if len(part_freq_train) else 0.0

Â  Â  df_train_f = attach_train_features(df_train, mtbf_train, part_freq_train, default_mtbf, default_freq)
Â  Â  df_calib_f = attach_train_features(df_calib, mtbf_train, part_freq_train, default_mtbf, default_freq)
Â  Â  df_test_fÂ  = attach_train_features(df_test, mtbf_train, part_freq_train, default_mtbf, default_freq)

Â  Â  # Use permanent rate-cols setting
Â  Â  X_train, y_train, FEATURES = make_xy(df_train_f, thr_label, USE_RATE_COLS_PERMANENT)
Â  Â  X_calib, y_calib, _Â  Â  Â  Â  = make_xy(df_calib_f, thr_label, USE_RATE_COLS_PERMANENT)
Â  Â  X_test,Â  y_test,Â  _Â  Â  Â  Â  = make_xy(df_test_f,Â  thr_label, USE_RATE_COLS_PERMANENT)

Â  Â  # Note: Use n_estimators from validation tab, if running prediction on its own, it uses DEFAULT_ESTIMATORS
Â  Â  n_est = st.session_state.validation_results.get("n_estimators", DEFAULT_ESTIMATORS)
Â  Â  _, calibrated_model, calib_method = train_and_calibrate(X_train, y_train, X_calib, y_calib, n_est)

Â  Â  p_calib = calibrated_model.predict_proba(X_calib)[:, 1] if len(X_calib) else np.array([])
Â  Â  p_testÂ  = calibrated_model.predict_proba(X_test)[:, 1]Â  if len(X_test) else np.array([])

Â  Â  # Guarded prior shift - NO LONGER USED ON PREDICT TAB, but calculation remains for validation
Â  Â  shift_note = "Prior shift is disabled on the Predict tab to avoid manual error. See Validation tab."

Â  Â  # Inputs
Â  Â  c1, c2, c3, c4 = st.columns(4)
Â  Â  with c1:
Â  Â  Â  Â  part_ids = sorted(df["part_id"].unique())
Â  Â  Â  Â  selected_part = st.selectbox("Select Part ID", part_ids)
Â  Â  with c2:
Â  Â  Â  Â  quantity = st.number_input("Order Quantity", 1, 100000, 351)
Â  Â  with c3:
Â  Â  Â  Â  weight = st.number_input("Piece Weight (lbs)", 0.1, 100.0, 4.0)
Â  Â  with c4:
Â  Â  Â  Â  cost_per_part = st.number_input("Cost per Part ($)", 0.01, 100.0, 20.00, 0.01) # Set default cost to 20.00
Â  Â Â 
Â  Â  # Recalculate Exceedance Baselines just before prediction
Â  Â  part_prev_train, part_scale, global_prev = compute_part_exceedance_baselines(df_train, thr_label)

Â  Â  mttf_value = float(mtbf_train.loc[selected_part, "mttf_scrap"]) if selected_part in mtbf_train.index else default_mtbf
Â  Â  part_freq_value = float(part_freq_train.get(selected_part, default_freq))

Â  Â  # Build an input row ALIGNED to FEATURES
Â  Â  input_row = build_input_row_for_part(
Â  Â  Â  Â  selected_part=selected_part,
Â  Â  Â  Â  quantity=quantity,
Â  Â  Â  Â  weight=weight,
Â  Â  Â  Â  mttf_value=mttf_value,
Â  Â  Â  Â  part_freq_value=part_freq_value,
Â  Â  Â  Â  FEATURES=FEATURES,
Â  Â  Â  Â  df_train=df_train_f
Â  Â  )

Â  Â  if st.button("Predict", type="primary", use_container_width=True):
Â  Â  Â  Â  # Base & adjusted predictions
Â  Â  Â  Â  base_p = float(calibrated_model.predict_proba(input_row)[0, 1])

Â  Â  Â  Â  adj_factor = float(part_scale.get(selected_part, 1.0)) ** float(gamma_star)
Â  Â  Â  Â  corrected_p = np.clip(base_p * float(s_star) * adj_factor, 0, 1)

Â  Â  Â  Â  expected_scrap_count = int(round(corrected_p * quantity))
Â  Â  Â  Â  expected_loss = round(expected_scrap_count * cost_per_part, 2)

Â  Â  Â  Â  # MTTF + reliability at current threshold
Â  Â  Â  Â  part_df = df_train[df_train["part_id"] == selected_part]
Â  Â  Â  Â  N = len(part_df)
Â  Â  Â  Â  failures = int((part_df["scrap%"] > thr_label).sum())
Â  Â  Â  Â  mttf_scrap = (N / failures) if failures > 0 else float("inf")
Â  Â  Â  Â  lam = 0.0 if mttf_scrap == float("inf") else 1.0 / mttf_scrap
Â  Â  Â  Â  reliability_next_run = np.exp(-lam * 1.0) if lam > 0 else 1.0

Â  Â  Â  Â  # Metrics
Â  Â  Â  Â  m1, m2, m3, m4 = st.columns(4)
Â  Â  Â  Â  m1.metric("Predicted Scrap Risk (raw)", f"{base_p*100:.2f}%")
Â  Â  Â  Â  m2.metric("Adjusted Scrap Risk (sÂ·part^Î³)", f"{corrected_p*100:.2f}%")
Â  Â  Â  Â  m3.metric("Expected Scrap Count", f"{expected_scrap_count} parts")
Â  Â  Â  Â  m4.metric("Expected Loss", f"${expected_loss:.2f}")

Â  Â  Â  Â  st.markdown(f"**Quick-hook params:** s = {s_star:.2f}, Î³ = {gamma_star:.2f} â€ƒ|â€ƒCalibration: **{calib_method}**")
Â  Â  Â  Â  st.caption(shift_note)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Generate and display the alert
Â  Â  Â  Â  color, status, message = get_alert_summary(corrected_p)

Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  f"""
Â  Â  Â  Â  Â  Â  <div style='
Â  Â  Â  Â  Â  Â  Â  Â  background-color:{color};
Â  Â  Â  Â  Â  Â  Â  Â  padding:1em;
Â  Â  Â  Â  Â  Â  Â  Â  border-radius:10px;
Â  Â  Â  Â  Â  Â  Â  Â  color:white;
Â  Â  Â  Â  Â  Â  Â  Â  font-weight:bold;
Â  Â  Â  Â  Â  Â  Â  Â  margin-top: 1em;
Â  Â  Â  Â  Â  Â  '>
Â  Â  Â  Â  Â  Â  Â  Â  {status}<br><span style='font-weight:normal'>{message}</span>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  """,
Â  Â  Â  Â  Â  Â  unsafe_allow_html=True
Â  Â  Â  Â  )

Â  Â  Â  Â  st.subheader("Reliability context (at current threshold)")
Â  Â  Â  Â  r1, r2, r3 = st.columns(3)
Â  Â  Â  Â  r1.metric("MTTFscrap", "âˆ runs" if mttf_scrap == float("inf") else f"{mttf_scrap:.2f} runs")
Â  Â  Â  Â  r2.metric("Reliability (next run)", f"{reliability_next_run*100:.2f}%")
Â  Â  Â  Â  r3.metric("Failures / Runs", f"{failures} / {N}")
Â  Â  Â  Â  st.caption("Reliability computed as R(1) = exp(âˆ’1/MTTFscrap). Threshold slider sets both labels and MTTF calculation.")

Â  Â  Â  Â  # Historical exceedance prevalence at current threshold
Â  Â  Â  Â  part_prev_card = float(part_prev_train.get(selected_part, np.nan)) if 'part_prev_train' in locals() else np.nan
Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  f"**Historical Exceedance Rate @ {thr_label:.1f}% (part):** "
Â  Â  Â  Â  Â  Â  f"{(part_prev_card*100 if not np.isnan(part_prev_card) else np.nan):.2f}% â€ƒ({N} runs)"
Â  Â  Â  Â  )
Â  Â  Â  Â  # Use simple comparison
Â  Â  Â  Â  if not np.isnan(part_prev_card): # Added check to ensure comparison is valid
Â  Â  Â  Â  Â  Â  Â if corrected_p > part_prev_card:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning("â¬†ï¸ Prediction above historical exceedance rate for this part.")
Â  Â  Â  Â  Â  Â  Â elif corrected_p < part_prev_card:
Â  Â  Â  Â  Â  Â  Â  Â  st.success("â¬‡ï¸ Prediction below historical exceedance rate for this part.")
Â  Â  Â  Â  Â  Â  Â else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("â‰ˆ Equal to historical exceedance rate.")
Â  Â  Â  Â Â 

Â  Â  Â  Â  # -----------------------------
Â  Â  Â  Â  # NEW: Historical vs Predicted Pareto (80% rule applied)
Â  Â  Â  Â  # -----------------------------
Â  Â  Â  Â  st.subheader("Pareto of Defects â€” Historical vs Predicted (Top 80% Drivers)")

Â  Â  Â  Â  rate_cols_in_model = [c for c in FEATURES if c.endswith("_rate")]
Â  Â  Â  Â  if not rate_cols_in_model:
Â  Â  Â  Â  Â  Â  Â st.error("Cannot compute Pareto. No *_rate features found in data or model.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # Historical Pareto for the selected part (now uses 80% rule)
Â  Â  Â  Â  Â  Â  hist_pareto = historical_defect_pareto_for_part(selected_part, df_train_f)
Â  Â  Â  Â  Â  Â  hist_pareto = hist_pareto.rename(columns={"mean_rate": "hist_mean_rate"})

Â  Â  Â  Â  Â  Â  # Local (predicted) drivers for the current input (now uses 80% rule)
Â  Â  Â  Â  Â  Â  pred_pareto = local_defect_drivers(
Â  Â  Â  Â  Â  Â  Â  Â  calibrated_model=calibrated_model,
Â  Â  Â  Â  Â  Â  Â  Â  input_row=input_row,
Â  Â  Â  Â  Â  Â  Â  Â  FEATURES=FEATURES,
Â  Â  Â  Â  Â  Â  Â  Â  df_train=df_train_f,
Â  Â  Â  Â  Â  Â  Â  Â  strategy="p75",
Â  Â  Â  Â  Â  Â  ).rename(columns={"delta_prob": "delta_prob_raw"})

Â  Â  Â  Â  Â  Â  # Left-right display
Â  Â  Â  Â  Â  Â  c_left, c_right = st.columns(2)
Â  Â  Â  Â  Â  Â  with c_left:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Historical Pareto (Top 80% defect rates for this part)**")
Â  Â  Â  Â  Â  Â  Â  Â  if len(hist_pareto):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hist_pareto.assign(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hist_mean_rate=lambda d: d["hist_mean_rate"].round(4)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ).assign(**{
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "share_%": lambda d: d["share_%"].round(2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "cumulative_%": lambda d: d["cumulative_%"].round(1),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("No historical defect rates found for this part in the training window.")

Â  Â  Â  Â  Â  Â  with c_right:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**Predicted Pareto (Top 80% local drivers of current prediction)**")
Â  Â  Â  Â  Â  Â  Â  Â  if len(pred_pareto):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pred_pareto.assign(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  delta_prob_raw=lambda d: (d["delta_prob_raw"]*100).round(2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ).assign(**{
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "share_%": lambda d: d["share_%"].round(2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "cumulative_%": lambda d: d["cumulative_%"].round(1),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }).rename(columns={"delta_prob_raw": "delta_prob_increase_pp"})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("No local defect drivers found that positively increase the scrap risk for this input.")
